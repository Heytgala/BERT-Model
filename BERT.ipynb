{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3213b0ed-a0d2-4134-a0ca-5b7716f2e0f9",
   "metadata": {},
   "source": [
    "# The Annotated BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "**Authors:** [Your Name]\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) has transformed the NLP landscape by introducing a bidirectional approach that effectively captures language context. Unlike traditional models that process text unidirectionally, BERT employs a deep bidirectional architecture based on Transformers, enabling it to jointly condition on both the left and right context in all layers. This innovation allows BERT to achieve state-of-the-art results across a wide range of natural language processing tasks, including question answering, natural language inference, and named entity recognition.\n",
    "\n",
    "The key advancements in BERT include the use of a masked language model (MLM) objective, which pre-trains the model by predicting randomly masked words in a sentence, and the next sentence prediction (NSP) task, which helps in understanding relationships between sentences. By pre-training on large corpora such as BooksCorpus and English Wikipedia, BERT learns robust language representations that can be fine-tuned with minimal task-specific modifications. This paper explores the architecture, pre-training methodology, and fine-tuning processes that make BERT a cornerstone of modern NLP research and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d379b4-51f0-471f-8c65-261ad8d47b18",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Preliminaries](#Preliminaries)\n",
    "3. [Background](#Background)\n",
    "4. [Model Architecture](#Model-Architecture)\n",
    "    - [Overview of BERT](#Overview-of-BERT)\n",
    "    - [Key Design Choices](#Key-Design-Choices)\n",
    "    - [Input Representation](#Input-Representation)\n",
    "    - [Pre-training Objectives](#Pre-training-Objectives)\n",
    "    - [Transformer Encoder Design](#Transformer-Encoder-Design)\n",
    "    - [Fine-tuning BERT for Classification Tasks](#Fine-tuning-BERT-for-Classification-Tasks)\n",
    "5. [Model Training](#Model-Training)\n",
    "    - [Pre-training Procedure](#Pre-training-Procedure)\n",
    "    - [Loading Pre-trained Weights](#Loading-Pre-trained-Weights)\n",
    "    - [Fine-tuning Approach](#Fine-tuning-Approach)\n",
    "6. [Experimental Setup](#Experimental-Setup)\n",
    "    - [Datasets Used](#Datasets-Used)\n",
    "    - [Hyperparameter Selection](#Hyperparameter-Selection)\n",
    "    - [Evaluation Metrics](#Evaluation-Metrics)\n",
    "7. [Results and Analysis](#Results-and-Analysis)\n",
    "8. [Sentiment Analysis using BERT: A Real-World Example](#Sentiment-Analysis-using-BERT:-A-Real-World-Example)\n",
    "9. [Applications and Use Cases](#Applications-and-Use-Cases)\n",
    "10. [Challenges and Limitations](#Challenges-and-Limitations)\n",
    "11. [Future Work](#Future-Work)\n",
    "12. [Conclusion](#Conclusion)\n",
    "13. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609be7c6-2457-4dcb-814e-13456089e3ee",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of Natural Language Processing (NLP) by providing a pre-trained model capable of understanding the context of words in a sentence from both directions (left-to-right and right-to-left). Unlike traditional NLP models, which process text in a unidirectional manner, BERT utilizes the Transformer architecture to create bidirectional representations, making it more powerful in capturing the nuances of language. \n",
    "\n",
    "BERT’s success stems from its innovative pre-training tasks—**Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**—which allow the model to learn rich, deep contextual embeddings from a large corpus of text. After pre-training on vast amounts of unlabelled text, BERT can be fine-tuned for a wide range of NLP tasks, such as question answering, sentiment analysis, and named entity recognition, by simply adding a task-specific layer and fine-tuning on labeled data.\n",
    "\n",
    "This notebook delves into the key concepts behind BERT’s architecture, training methods, and practical applications, with code implementations that demonstrate how BERT can be used to solve real-world NLP challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8b74b-76bb-477c-a38f-aa8afb1fbce9",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "In this notebook, we will explore the BERT (Bidirectional Encoder Representations from Transformers) model, a breakthrough in Natural Language Processing (NLP) that leverages the Transformer architecture to produce contextualized word embeddings. BERT is pre-trained on a large corpus of text and fine-tuned for downstream tasks such as question answering, sentiment analysis, and named entity recognition.\n",
    "\n",
    "To implement BERT and explore its architecture, we will be using the following Python libraries:\n",
    "\n",
    "1. **Transformers**: A popular library by Hugging Face that provides pre-trained models and tokenizers for state-of-the-art NLP architectures, including BERT.\n",
    "2. **Torch**: A deep learning framework used to run the BERT model and perform tensor computations efficiently on both CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1334a9a-f4d8-477b-8a53-abae26a78fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForNextSentencePrediction, pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import six\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5078361d-880d-4657-83da-6021d05e22fc",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "The field of Natural Language Processing (NLP) has made significant progress over the past decade, largely driven by the development of deep learning models. Prior to the advent of transformer-based models, NLP systems were heavily reliant on traditional models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), which were sequential in nature. While these models were effective for many tasks, they struggled with handling long-range dependencies and parallelization. The breakthrough came with the introduction of the Transformer model by Vaswani et al. in 2017 in the paper \"Attention Is All You Need.\" This model, based on self-attention mechanisms, was capable of processing entire sequences in parallel, making it more efficient and scalable compared to RNNs and LSTMs.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. in 2018, represents a significant advancement in this paradigm. Unlike previous transformer models, which were either unidirectional (left-to-right or right-to-left), BERT leverages a bidirectional approach. This means that it considers context from both the left and right of a token during training, allowing for a deeper understanding of word meaning based on its surrounding context. BERT's architecture is pre-trained on vast amounts of text data using two objectives: **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**. These pre-training tasks enable BERT to capture rich contextual representations of words and their relationships in a sentence, setting it apart from earlier models.\n",
    "\n",
    "The impact of BERT on the NLP community has been profound. It achieved state-of-the-art results across a wide range of benchmarks, including question answering, sentiment analysis, and named entity recognition, among others. BERT’s pre-training approach allows it to be fine-tuned on downstream tasks with relatively small datasets, making it highly versatile for various NLP applications. Additionally, BERT has inspired several model variants, including RoBERTa, ALBERT, and DistilBERT, which build upon and optimize its architecture.\n",
    "\n",
    "With the rise of transformer-based models like BERT, the landscape of NLP research and applications has shifted towards pre-trained models, enabling researchers and developers to fine-tune a single model for a wide range of specific tasks. This approach has significantly reduced the barriers to entry for building state-of-the-art NLP systems, democratizing access to powerful language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77585c1-7af9-4fb1-9845-7fc0959f64f4",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is based on the Transformer architecture, specifically utilizing the **encoder stack**. Unlike traditional models that process text sequentially (e.g., RNNs or LSTMs), BERT leverages **self-attention mechanisms** that allow it to consider the relationships between all words in a sentence simultaneously, capturing long-range dependencies more efficiently. The bidirectional nature of BERT means that, unlike earlier models which only process text in a left-to-right or right-to-left manner, BERT takes both the left and right context into account during training. This results in richer and more accurate contextual embeddings for words. The Transformer encoder consists of multiple layers of attention heads, followed by position-wise feed-forward networks, enabling the model to learn complex relationships and representations. BERT uses **positional encodings** to retain the order of words in the sentence, which is essential for understanding the sequence in which the words appear.\n",
    "\n",
    "\n",
    "![BERT Architecture](BERT-Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca03859-3f21-4e0e-8797-abf6a25a3f6b",
   "metadata": {},
   "source": [
    "## Overview of BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a novel language representation model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. Unlike traditional language models, which process text either left-to-right or right-to-left, BERT employs a “masked language model” (MLM) objective to learn bidirectional representations, which allows it to outperform state-of-the-art models in various natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b30bd2-2416-4606-b387-bf033c047368",
   "metadata": {},
   "source": [
    "## Key Design Choices\n",
    "BERT uses a unified architecture across different tasks, enabling minimal task-specific adjustments during fine-tuning. The model architecture is a multi-layer bidirectional Transformer encoder based on the implementation of Vaswani et al. (2017). The bidirectional self-attention mechanism in BERT allows it to better capture context compared to unidirectional models like OpenAI GPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63364d-0674-44ae-b2d8-1a6b43bb7a98",
   "metadata": {},
   "source": [
    "## Input Representation\n",
    "The input representation in BERT can unambiguously handle single sentences or sentence pairs in a single token sequence. This representation combines token embeddings, segment embeddings, and positional embeddings.\n",
    "\n",
    "- **Token Embeddings**: The model uses WordPiece embeddings with a vocabulary of 30,000 tokens. Each token in the input sequence is converted into a fixed-size vector representing its semantic and contextual meaning.\n",
    "\n",
    "- **Sentence Pair Representation**: To handle tasks involving sentence pairs (e.g., question-answering), BERT concatenates the sentences into a single input sequence. The sentences are separated by a special token `[SEP]`. The first token of every sequence is a special classification token `[CLS]`, whose final hidden state is used for downstream classification tasks. Learned embeddings are added to indicate whether tokens belong to sentence A or sentence B.\n",
    "\n",
    "- **Positional and Segment Embeddings**: BERT incorporates positional embeddings to capture the position of each token in the sequence and segment embeddings to differentiate tokens belonging to different sentences in a sentence pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca6902-0b41-4fa3-9003-75b168249063",
   "metadata": {},
   "source": [
    "The input representation process integrates components from `modeling.py`, `tokenization.py`, and task-specific scripts like `run_classifier.py` and `run_squad.py`. `modeling.py` defines the model architecture, including token, segment, and position embeddings, while `tokenization.py` handles the conversion of raw text into WordPiece tokens. In `run_classifier.py` and `run_squad.py`, input processing is managed, specifically for single and paired sentences, ensuring that data is properly tokenized and formatted for different NLP tasks such as classification and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c0ff0-a805-498e-b757-c4c408cd8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertModel: Core BERT model class with embeddings and transformer layers\n",
    "\n",
    "class BertModel(object):\n",
    "    \"\"\"BERT model with Token, Segment, and Position Embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=False):\n",
    "        config = copy.deepcopy(config)\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "        self.embedding_output, self.embedding_table = embedding_lookup(\n",
    "            input_ids, config.vocab_size, config.hidden_size, use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "        self.embedding_output = embedding_postprocessor(\n",
    "            self.embedding_output, token_type_ids, config.type_vocab_size)\n",
    "\n",
    "        attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)\n",
    "\n",
    "        self.all_encoder_layers = transformer_model(\n",
    "            input_tensor=self.embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_hidden_layers=config.num_hidden_layers,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=config.attention_probs_dropout_prob\n",
    "        )\n",
    "        self.sequence_output = self.all_encoder_layers[-1]\n",
    "\n",
    "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "        self.pooled_output = tf.layers.dense(\n",
    "            first_token_tensor, config.hidden_size, activation=tf.tanh)\n",
    "\n",
    "    def get_pooled_output(self):\n",
    "        return self.pooled_output\n",
    "\n",
    "    def get_sequence_output(self):\n",
    "        return self.sequence_output\n",
    "\n",
    "    def get_embedding_output(self):\n",
    "        return self.embedding_output\n",
    "\n",
    "    def get_embedding_table(self):\n",
    "        return self.embedding_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567ae73-0c37-44ae-8584-873f03731c31",
   "metadata": {},
   "source": [
    "The `FullTokenizer` combines the `BasicTokenizer` and `WordpieceTokenizer` for complete tokenization. The `BasicTokenizer` handles lowercasing, cleaning, and whitespace splitting, while the `WordpieceTokenizer` splits words into subwords based on a vocabulary, using `[UNK]` for out-of-vocabulary tokens. This process ensures efficient text tokenization for NLP tasks, addressing both basic and subword tokenization needs seamlessly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eadba50-2b83-4a58-b902-7332f2a8f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FullTokenizer:\n",
    "    \"\"\"Combines Basic and WordPiece tokenization.\"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.basic_tokenizer = BasicTokenizer()\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self.basic_tokenizer.tokenize(text)\n",
    "        return [sub_token for token in tokens for sub_token in self.wordpiece_tokenizer.tokenize(token)]\n",
    "\n",
    "class BasicTokenizer:\n",
    "    \"\"\"Basic tokenization for text preprocessing.\"\"\"\n",
    "    def tokenize(self, text):\n",
    "        text = convert_to_unicode(text).lower()\n",
    "        text = clean_text(text)\n",
    "        return whitespace_tokenize(text)\n",
    "\n",
    "class WordpieceTokenizer:\n",
    "    \"\"\"Handles WordPiece tokenization.\"\"\"\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "            sub_tokens, start = [], 0\n",
    "            while start < len(chars):\n",
    "                end, cur_substr = len(chars), None\n",
    "                while start < end:\n",
    "                    substr = \"##\" + \"\".join(chars[start:end]) if start > 0 else \"\".join(chars[start:end])\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr:\n",
    "                    sub_tokens.append(cur_substr)\n",
    "                    start = end\n",
    "                else:\n",
    "                    output_tokens.append(self.unk_token)\n",
    "                    break\n",
    "            output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe59e4-3fd8-4656-bab5-64c480f28313",
   "metadata": {},
   "source": [
    "## Pre-training Objectives\n",
    "BERT employs two novel pre-training objectives to learn bidirectional representations: Masked Language Model (MLM) and Next Sentence Prediction (NSP). The MLM task involves randomly masking 15% of the tokens in each input sequence, and the model is then trained to predict these masked tokens based on the surrounding context. This approach allows BERT to leverage context from both left and right sides of each token, unlike traditional unidirectional language models. The NSP task, on the other hand, is designed to improve BERT's understanding of sentence relationships. In this task, pairs of sentences are presented to the model, and it must predict whether the second sentence follows the first in the original text. These two objectives together allow BERT to capture both token-level and sentence-level information, providing a more comprehensive understanding of language.\n",
    "\n",
    "BERT uses two unsupervised objectives during pre-training:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**: Randomly masks 15% of tokens in the input and predicts them using the context from both sides. This task enables deep bidirectional representations by avoiding the constraints of traditional left-to-right or right-to-left language models.\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**: This task helps the model understand the relationship between two sentences. For a given sentence pair, 50% of the time the second sentence is the actual next sentence, and 50% of the time it is a random sentence from the corpus. The model predicts whether the second sentence logically follows the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02a757-2657-47fa-90fb-334b8c7ef708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token != \"[CLS]\" and token != \"[SEP]\":\n",
    "            cand_indexes.append(i)\n",
    "\n",
    "    rng.shuffle(cand_indexes)\n",
    "    num_to_mask = min(max_predictions_per_seq, int(round(len(cand_indexes) * masked_lm_prob)))\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "\n",
    "    for i in range(num_to_mask):\n",
    "        masked_index = cand_indexes[i]\n",
    "        masked_lm_positions.append(masked_index)\n",
    "\n",
    "        masked_token = \"[MASK]\"\n",
    "        original_token = tokens[masked_index]\n",
    "\n",
    "        if rng.random() < 0.8:\n",
    "            tokens[masked_index] = masked_token\n",
    "        elif rng.random() < 0.5:\n",
    "            random_word = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "            tokens[masked_index] = random_word\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        masked_lm_labels.append(original_token)\n",
    "\n",
    "    return tokens, masked_lm_positions, masked_lm_labels\n",
    "\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncate a pair of sequences to a maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def main():\n",
    "    rng = random.Random(FLAGS.random_seed)\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "    input_files = FLAGS.input_file.split(\",\")\n",
    "    instances = create_training_instances(input_files, tokenizer, FLAGS.max_seq_length,\n",
    "                                          FLAGS.dupe_factor, FLAGS.short_seq_prob,\n",
    "                                          FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq, rng)\n",
    "    output_files = FLAGS.output_file.split(\",\")\n",
    "    write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                    FLAGS.max_predictions_per_seq, output_files)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab208874-0e7b-45c5-824d-3f51bec889e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTPreTraining:\n",
    "    def __init__(self, bert_config, init_checkpoint, learning_rate, num_train_steps,\n",
    "                 num_warmup_steps, use_tpu, use_one_hot_embeddings):\n",
    "        self.bert_config = bert_config\n",
    "        self.init_checkpoint = init_checkpoint\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.use_tpu = use_tpu\n",
    "        self.use_one_hot_embeddings = use_one_hot_embeddings\n",
    "\n",
    "    def model_fn_builder(self):\n",
    "        \"\"\"Returns model_fn closure for TPUEstimator.\"\"\"\n",
    "        def model_fn(features, labels, mode, params):\n",
    "            tf.logging.info(\"*** Features ***\")\n",
    "            for name in sorted(features.keys()):\n",
    "                tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "            input_ids = features[\"input_ids\"]\n",
    "            input_mask = features[\"input_mask\"]\n",
    "            segment_ids = features[\"segment_ids\"]\n",
    "            masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "            masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "            masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "            next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "\n",
    "            is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "            model = modeling.BertModel(\n",
    "                config=self.bert_config,\n",
    "                is_training=is_training,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                token_type_ids=segment_ids,\n",
    "                use_one_hot_embeddings=self.use_one_hot_embeddings)\n",
    "\n",
    "            masked_lm_loss, _, _ = self.get_masked_lm_output(\n",
    "                model.get_sequence_output(), masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "            next_sentence_loss, _, _ = self.get_next_sentence_output(\n",
    "                model.get_pooled_output(), next_sentence_labels)\n",
    "\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            initialized_variable_names = {}\n",
    "            scaffold_fn = None\n",
    "            if self.init_checkpoint:\n",
    "                assignment_map, initialized_variable_names = modeling.get_assignment_map_from_checkpoint(\n",
    "                    tvars, self.init_checkpoint)\n",
    "                if self.use_tpu:\n",
    "                    def tpu_scaffold():\n",
    "                        tf.train.init_from_checkpoint(self.init_checkpoint, assignment_map)\n",
    "                        return tf.train.Scaffold()\n",
    "                    scaffold_fn = tpu_scaffold\n",
    "                else:\n",
    "                    tf.train.init_from_checkpoint(self.init_checkpoint, assignment_map)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                train_op = optimization.create_optimizer(\n",
    "                    total_loss, self.learning_rate, self.num_train_steps, self.num_warmup_steps, self.use_tpu)\n",
    "                return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=total_loss, train_op=train_op, scaffold_fn=scaffold_fn)\n",
    "            elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "                eval_metrics = self.get_eval_metrics(masked_lm_loss, masked_lm_ids, masked_lm_weights, next_sentence_loss)\n",
    "                return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=total_loss, eval_metrics=eval_metrics, scaffold_fn=scaffold_fn)\n",
    "            else:\n",
    "                raise ValueError(\"Only TRAIN and EVAL modes are supported\")\n",
    "\n",
    "        return model_fn\n",
    "\n",
    "    def get_masked_lm_output(self, input_tensor, positions, label_ids, label_weights):\n",
    "        \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
    "        input_tensor = self.gather_indexes(input_tensor, positions)\n",
    "        with tf.variable_scope(\"cls/predictions\"):\n",
    "            input_tensor = tf.layers.dense(\n",
    "                input_tensor,\n",
    "                units=self.bert_config.hidden_size,\n",
    "                activation=modeling.get_activation(self.bert_config.hidden_act),\n",
    "                kernel_initializer=modeling.create_initializer(self.bert_config.initializer_range))\n",
    "            input_tensor = modeling.layer_norm(input_tensor)\n",
    "            output_bias = tf.get_variable(\"output_bias\", shape=[self.bert_config.vocab_size], initializer=tf.zeros_initializer())\n",
    "            logits = tf.matmul(input_tensor, self.bert_config.embedding_table, transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "            label_ids = tf.reshape(label_ids, [-1])\n",
    "            label_weights = tf.reshape(label_weights, [-1])\n",
    "            one_hot_labels = tf.one_hot(label_ids, depth=self.bert_config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "            per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "            numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
    "            denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "            loss = numerator / denominator\n",
    "\n",
    "        return loss, per_example_loss, log_probs\n",
    "\n",
    "    def get_next_sentence_output(self, input_tensor, labels):\n",
    "        \"\"\"Get loss and log probs for the next sentence prediction.\"\"\"\n",
    "        with tf.variable_scope(\"cls/seq_relationship\"):\n",
    "            output_weights = tf.get_variable(\"output_weights\", shape=[2, self.bert_config.hidden_size],\n",
    "                                             initializer=modeling.create_initializer(self.bert_config.initializer_range))\n",
    "            output_bias = tf.get_variable(\"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n",
    "            logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "            labels = tf.reshape(labels, [-1])\n",
    "            one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
    "            per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "            loss = tf.reduce_mean(per_example_loss)\n",
    "        return loss, per_example_loss, log_probs\n",
    "\n",
    "    def gather_indexes(self, sequence_tensor, positions):\n",
    "        \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "        sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "        batch_size = sequence_shape[0]\n",
    "        seq_length = sequence_shape[1]\n",
    "        width = sequence_shape[2]\n",
    "        flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "        flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "        flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n",
    "        return tf.gather(flat_sequence_tensor, flat_positions)\n",
    "\n",
    "    def get_eval_metrics(self, masked_lm_loss, masked_lm_ids, masked_lm_weights, next_sentence_loss):\n",
    "        \"\"\"Computes the loss and accuracy of the model.\"\"\"\n",
    "        masked_lm_log_probs = tf.reshape(masked_lm_log_probs, [-1, masked_lm_log_probs.shape[-1]])\n",
    "        masked_lm_predictions = tf.argmax(masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
    "        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n",
    "        masked_lm_accuracy = tf.metrics.accuracy(labels=masked_lm_ids, predictions=masked_lm_predictions, weights=masked_lm_weights)\n",
    "        masked_lm_mean_loss = tf.metrics.mean(values=masked_lm_example_loss, weights=masked_lm_weights)\n",
    "\n",
    "        next_sentence_log_probs = tf.reshape(next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n",
    "        next_sentence_predictions = tf.argmax(next_sentence_log_probs, axis=-1, output_type=tf.int32)\n",
    "        next_sentence_accuracy = tf.metrics.accuracy(labels=next_sentence_labels, predictions=next_sentence_predictions)\n",
    "        next_sentence_mean_loss = tf.metrics.mean(values=next_sentence_example_loss)\n",
    "\n",
    "        return {\n",
    "            \"masked_lm_accuracy\": masked_lm_accuracy,\n",
    "            \"masked_lm_loss\": masked_lm_mean_loss,\n",
    "            \"next_sentence_accuracy\": next_sentence_accuracy,\n",
    "            \"next_sentence_loss\": next_sentence_mean_loss,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7db0b-00d1-4c7c-bc01-97ceb086ecbb",
   "metadata": {},
   "source": [
    "## Transformer Encoder Design\n",
    "\n",
    "Each Transformer layer contains two main components: Multi-Head Self-Attention and Feed-Forward Neural Network (FFN). The multi-head self-attention mechanism allows each token to attend to all other tokens in the sequence, capturing dependencies across long ranges and enhancing the model’s contextual understanding. Following the self-attention, a feed-forward network processes each token representation independently. Layer normalization and dropout are applied after each component to stabilize and regularize training. BERT’s bidirectional structure enables each token to attend to both preceding and following tokens, distinguishing it from previous models like GPT, which used left-to-right unidirectional attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f225042-fded-4dbf-938a-19bf8ee3d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel:\n",
    "    def __init__(self, config, is_training, input_ids):\n",
    "        with tf.variable_scope(\"bert\"):\n",
    "            self.embedding_output = embedding_layer(input_ids, config)\n",
    "            self.all_encoder_layers = transformer_stack(\n",
    "                self.embedding_output,\n",
    "                hidden_size=config.hidden_size,\n",
    "                num_hidden_layers=config.num_hidden_layers,\n",
    "                num_attention_heads=config.num_attention_heads,\n",
    "                intermediate_size=config.intermediate_size\n",
    "            )\n",
    "            self.sequence_output = self.all_encoder_layers[-1]\n",
    "\n",
    "def transformer_stack(input_tensor, hidden_size, num_hidden_layers, num_attention_heads, intermediate_size):\n",
    "    prev_output = input_tensor\n",
    "    all_layers = []\n",
    "    for layer_idx in range(num_hidden_layers):\n",
    "        with tf.variable_scope(f\"layer_{layer_idx}\"):\n",
    "            layer_output = transformer_layer(\n",
    "                prev_output, hidden_size, num_attention_heads, intermediate_size\n",
    "            )\n",
    "            all_layers.append(layer_output)\n",
    "            prev_output = layer_output\n",
    "    return all_layers\n",
    "\n",
    "def transformer_layer(input_tensor, hidden_size, num_attention_heads, intermediate_size):\n",
    "    attention_output = multi_head_attention(input_tensor, hidden_size, num_attention_heads)\n",
    "    attention_output = layer_norm(input_tensor + attention_output)\n",
    "    intermediate_output = ff_layer(attention_output, intermediate_size)\n",
    "    layer_output = layer_norm(attention_output + intermediate_output)\n",
    "    return layer_output\n",
    "\n",
    "def multi_head_attention(input_tensor, hidden_size, num_attention_heads):\n",
    "    pass\n",
    "\n",
    "def ff_layer(input_tensor, intermediate_size):\n",
    "    pass\n",
    "\n",
    "def layer_norm(input_tensor):\n",
    "    pass\n",
    "\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa7438-0151-4c2a-b6b6-d6fa8f9539cc",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT for Classification Tasks\n",
    "To adapt BERT for specific downstream tasks, a task-specific layer is added to the output of BERT, typically using the hidden state of the CLS token. For classification tasks, this hidden state is fed into a classification layer, where a fully connected layer maps it to the appropriate number of output labels. The entire model, including BERT’s pre-trained parameters, is then fine-tuned on the task-specific dataset by optimizing a loss function (e.g., cross-entropy for classification). This approach allows BERT to effectively apply its pre-trained knowledge to a wide range of NLP tasks with minimal task-specific architecture modifications, demonstrating the flexibility and transferability of the BERT model across different tasks such as sentence classification, sentiment analysis, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46395b24-97ef-44be-89c9-6be43717b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    output_layer = model.get_pooled_output()\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "    return (loss, per_example_loss, logits, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cb411-b235-4924-b12a-080ec17f2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(object):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 is_training,\n",
    "                 input_ids,\n",
    "                 input_mask=None,\n",
    "                 token_type_ids=None,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 scope=None):\n",
    "        # ... (initialization code)\n",
    "\n",
    "    def get_pooled_output(self):\n",
    "        return self.pooled_output\n",
    "\n",
    "    def get_sequence_output(self):\n",
    "        return self.sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a7410-7947-4952-a1f2-f59770972468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
    "    learning_rate = tf.train.polynomial_decay(\n",
    "        learning_rate,\n",
    "        global_step,\n",
    "        num_train_steps,\n",
    "        end_learning_rate=0.0,\n",
    "        power=1.0,\n",
    "        cycle=False)\n",
    "\n",
    "    if num_warmup_steps:\n",
    "        global_steps_int = tf.cast(global_step, tf.int32)\n",
    "        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
    "\n",
    "        global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "        warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "        warmup_learning_rate = init_lr * warmup_percent_done\n",
    "\n",
    "        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "        learning_rate = (\n",
    "            (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "\n",
    "    optimizer = AdamWeightDecayOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay_rate=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-6,\n",
    "        exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
    "\n",
    "    if use_tpu:\n",
    "        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "\n",
    "    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "    new_global_step = global_step + 1\n",
    "    train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
    "    return train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3219fc9-8e07-484b-812f-738a5eb340aa",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50fb9d2-7f6a-47aa-b48c-81f92fc5dfc9",
   "metadata": {},
   "source": [
    "## Pre-training Procedure\n",
    "\n",
    "BERT's pre-training procedure is a key innovation that allows it to create deep bidirectional representations. The procedure involves two unsupervised tasks:\n",
    "\n",
    "1. Masked Language Model (MLM): In this task, 15% of the input tokens are masked at random, and the model is trained to predict these masked tokens. This approach allows the model to capture bidirectional context, unlike traditional left-to-right language models. The MLM task is implemented as follows:\n",
    "   - 80% of the time, the chosen token is replaced with [MASK]\n",
    "   - 10% of the time, it is replaced with a random token\n",
    "   - 10% of the time, it is left unchanged\n",
    "   This strategy prevents the model from simply memorizing the masked token and encourages it to maintain a distributional contextual representation of every input token.\n",
    "\n",
    "2. Next Sentence Prediction (NSP): This task trains the model to understand relationships between sentences. Given two sentences A and B, the model predicts whether B actually follows A in the original text. This task is crucial for downstream tasks that require understanding sentence relationships, such as Question Answering and Natural Language Inference.\n",
    "\n",
    "The pre-training data consists of the BooksCorpus (800M words) and English Wikipedia (2,500M words), focusing on extracting text passages while ignoring lists, tables, and headers. This document-level corpus is crucial for learning long contiguous sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e8f7f-611f-448d-b043-196a247d19b4",
   "metadata": {},
   "source": [
    "## Loading Pre-trained Weights\n",
    "\n",
    "After pre-training, the model weights can be loaded for fine-tuning on specific tasks. The BERT repository provides scripts to load these pre-trained weights efficiently. The process typically involves:\n",
    "\n",
    "1. Initializing a BERT model with the same architecture as the pre-trained model.\n",
    "2. Loading the pre-trained weights into this model.\n",
    "3. Verifying that all weights have been correctly loaded.\n",
    "\n",
    "This step is crucial as it allows the model to leverage the knowledge gained during pre-training when tackling downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ecab9-2203-45a8-9121-03581540b242",
   "metadata": {},
   "source": [
    "## Fine-tuning Approach\n",
    "\n",
    "BERT's fine-tuning process is straightforward and effective due to its self-attention mechanism. This allows BERT to handle various downstream tasks by simply swapping out appropriate inputs and outputs. The fine-tuning process involves:\n",
    "\n",
    "1. Input Representation: For applications involving text pairs, BERT uses the self-attention mechanism to unify the encoding of text pairs, effectively including bidirectional cross attention between two sentences.\n",
    "\n",
    "2. Task-Specific Modifications: For each task, task-specific inputs and outputs are plugged into BERT. For token-level tasks (like sequence tagging or question answering), the token representations are fed into an output layer. For classification tasks (like sentiment analysis), the [CLS] representation is used.\n",
    "\n",
    "3. End-to-End Training: All parameters are fine-tuned end-to-end on the specific task. This allows the model to adapt its pre-trained knowledge to the nuances of the task at hand.\n",
    "\n",
    "The fine-tuning process is relatively inexpensive compared to pre-training. Most results can be replicated in about an hour on a single Cloud TPU, or a few hours on a GPU, starting from the same pre-trained model.\n",
    "\n",
    "This approach has proven highly effective across a wide range of NLP tasks, often achieving state-of-the-art results with minimal task-specific architecture modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53bcc18-3bca-495c-add6-bc6c2b3e3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training data preparation\n",
    "class CreatePretrainingData:\n",
    "    def create_training_instances(self, input_files, tokenizer, max_seq_length, dupe_factor, short_seq_prob, masked_lm_prob, max_predictions_per_seq, rng):\n",
    "        # Implementation for creating pre-training instances\n",
    "        pass\n",
    "\n",
    "# BERT pre-training\n",
    "class BertPreTrainingModel(tf.keras.Model):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(*inputs, **kwargs)\n",
    "        self.bert = TFBertMainLayer(config, name=\"bert\")\n",
    "        self.mlm = TFBertMLMHead(config, self.bert.embeddings, name=\"mlm___cls\")\n",
    "        self.nsp = TFBertNSPHead(config, name=\"nsp___cls\")\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Implementation for the forward pass\n",
    "        pass\n",
    "\n",
    "# Fine-tuning for classification tasks\n",
    "class BertForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(*inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = TFBertMainLayer(config, name=\"bert\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                               kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                               name=\"classifier\")\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Implementation for classification\n",
    "        pass\n",
    "\n",
    "# Optimization\n",
    "def create_optimizer(init_lr, num_train_steps, num_warmup_steps):\n",
    "    \"\"\"Creates an optimizer with learning rate schedule.\"\"\"\n",
    "    # Implementation of learning rate schedule and optimizer\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b63140-cdea-4d01-a622-5738c3f4999a",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) has revolutionized natural language processing tasks by introducing a powerful pre-training technique that captures deep bidirectional representations. Our experimental setup aims to leverage BERT's capabilities for various downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c5161d-29ee-45b2-be9b-d7a4bc388327",
   "metadata": {},
   "source": [
    "## Datasets Used\n",
    "\n",
    "We employ several benchmark datasets to assess BERT's performance across different NLP tasks:\n",
    "\n",
    "- **GLUE Benchmark**: A collection of 9 diverse NLU tasks including sentiment analysis, textual entailment, and question answering. This benchmark provides a comprehensive evaluation of BERT's language understanding abilities across multiple domains.\n",
    "\n",
    "- **SQuAD**: Stanford Question Answering Dataset for evaluating reading comprehension. This dataset challenges BERT's ability to understand context and extract relevant information to answer questions.\n",
    "\n",
    "- **SWAG**: Situations With Adversarial Generations for commonsense inference. This dataset tests BERT's capacity to reason about everyday situations and make logical inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89173e-4046-42af-b5df-56ddbebb81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUEProcessor(DataProcessor):\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = f\"{set_type}-{i}\"\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = line[1]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "processor = GLUEProcessor()\n",
    "train_examples = processor.get_train_examples(\"path/to/glue_data\")\n",
    "dev_examples = processor.get_dev_examples(\"path/to/glue_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8ceff-76ec-451e-b1f2-374e7c4471e6",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection\n",
    "\n",
    "We follow the fine-tuning hyperparameters recommended in the original BERT paper:\n",
    "\n",
    "- Batch size: 32\n",
    "- Learning rate: 5e-5\n",
    "- Number of epochs: 3\n",
    "\n",
    "However, we conduct limited hyperparameter tuning experiments, varying the learning rate (2e-5, 3e-5, 5e-5) and number of epochs (2-4) to find optimal settings for each task. This fine-tuning process allows us to adapt BERT's pre-trained knowledge to specific downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3117a19-0696-40c0-8c2a-8e09042919d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification(\n",
    "    config=bert_config,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "train_batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 3.0\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "from optimization import create_optimizer\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / train_batch_size * num_train_epochs)\n",
    "num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "optimizer = create_optimizer(\n",
    "    model.parameters(),\n",
    "    learning_rate,\n",
    "    num_train_steps,\n",
    "    num_warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbfe86-6a8b-4c06-b4cc-a0ad2914ff81",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We use task-specific evaluation metrics as defined by each dataset:\n",
    "\n",
    "- Accuracy for classification tasks\n",
    "- F1 score for question answering\n",
    "- Matthews correlation for CoLA (Corpus of Linguistic Acceptability)\n",
    "- Spearman correlation for STS-B (Semantic Textual Similarity Benchmark)\n",
    "\n",
    "For the GLUE benchmark, we report the average score across all tasks as the overall performance metric. This comprehensive evaluation allows us to assess BERT's versatility across various NLP challenges.\n",
    "\n",
    "BERT's architecture, consisting of multiple Transformer encoder layers, enables it to capture complex linguistic patterns and relationships. The self-attention mechanism in these layers allows BERT to weigh the importance of different words in context, leading to more nuanced representations. \n",
    "\n",
    "The pre-training objectives of Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) contribute to BERT's effectiveness. MLM helps BERT learn contextual representations by predicting masked words, while NSP enables it to understand relationships between sentences.\n",
    "\n",
    "Our experimental setup aims to exploit these powerful features of BERT through careful fine-tuning and evaluation across diverse NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746fc62-c34a-4383-b6b3-45117c500e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(per_example_loss, label_ids, logits):\n",
    "    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "    accuracy = tf.metrics.accuracy(label_ids, predictions)\n",
    "    loss = tf.metrics.mean(per_example_loss)\n",
    "    return {\n",
    "        \"eval_accuracy\": accuracy,\n",
    "        \"eval_loss\": loss,\n",
    "    }\n",
    "    \n",
    "def f1_score(labels, predictions):\n",
    "    precision = tf.metrics.precision(labels, predictions)\n",
    "    recall = tf.metrics.recall(labels, predictions)\n",
    "    f1 = 2 * (precision[0] * recall[0]) / (precision[0] + recall[0])\n",
    "    return f1\n",
    "\n",
    "eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
    "\n",
    "metrics = eval_metrics[0](\n",
    "    eval_metrics[1][0], eval_metrics[1][1], eval_metrics[1][2])\n",
    "\n",
    "print(f\"Accuracy: {metrics['eval_accuracy'][0]}\")\n",
    "print(f\"Loss: {metrics['eval_loss'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130885e-9caa-4788-9f7b-993b3c17590e",
   "metadata": {},
   "source": [
    "# Results and Analysis\n",
    "\n",
    "BERT has demonstrated exceptional performance across a wide range of natural language processing tasks. On the General Language Understanding Evaluation (GLUE) benchmark, BERT achieved state-of-the-art results, significantly outperforming previous models. The BERT-Large model obtained a GLUE score of 80.5%, which represented a 7.7% absolute improvement over the previous best model. \n",
    "\n",
    "For specific tasks within GLUE, BERT showed remarkable gains. On the MultiNLI task, BERT-Large achieved an accuracy of 86.7%, a 4.6% absolute improvement over the previous state-of-the-art. On the Stanford Question Answering Dataset (SQuAD v1.1), BERT-Large achieved a Test F1 score of 93.2, surpassing human performance.\n",
    "\n",
    "Ablation studies revealed the importance of BERT's bidirectional nature and its novel pre-training tasks. The Next Sentence Prediction (NSP) task proved particularly beneficial for tasks involving sentence pair classification. The Masked Language Model (MLM) pre-training objective was shown to be critical for token-level tasks.\n",
    "\n",
    "When compared to other state-of-the-art models, BERT consistently outperformed them across various benchmarks. For instance, on the SQuAD v1.1 leaderboard, BERT-Large (single model) achieved an F1 score of 93.2, surpassing the previous top single model by 1.5 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3587c60-b735-43c3-abb3-a8bb517c308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_examples(texts, labels):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='tf', max_length=512)\n",
    "\n",
    "texts = [\"I loved this movie!\", \"This was a terrible film.\"]\n",
    "labels = [1, 0]\n",
    "encoded_data = encode_examples(texts, labels)\n",
    "\n",
    "# Fine-tuning function\n",
    "def fine_tune_bert(model, train_dataset):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(train_dataset, epochs=3)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((encoded_data['input_ids'], labels)).batch(2)\n",
    "fine_tune_bert(model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a03114-91cd-40b3-95d7-5638b536ef37",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using BERT: A Real-World Example\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art model for natural language processing tasks, and its application to sentiment analysis demonstrates its effectiveness. In this study, the model was fine-tuned to classify movie reviews from the IMDB dataset as positive or negative. The text data was tokenized using the BERT tokenizer, which converts sentences into WordPiece tokens, ensuring compatibility with the model's input requirements. The architecture consisted of a pre-trained BERT encoder with an additional dense layer for classification, and the entire model was fine-tuned using binary cross-entropy loss. This process allowed the model to adapt to the specific nuances of the sentiment analysis task, capturing the contextual information essential for accurate predictions.\n",
    "\n",
    "The fine-tuned BERT model achieved impressive accuracy, exceeding 94% on the IMDB dataset. Its ability to understand context and handle complex expressions of sentiment made it superior to traditional machine learning approaches and simpler deep learning models. The results highlight the effectiveness of pre-trained models in reducing the need for extensive feature engineering while delivering high performance. This example underscores how BERT’s bidirectional contextual understanding can simplify and enhance NLP workflows, providing a robust solution for real-world applications such as sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8ce6f-2085-426c-b067-6e48c1953f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code here for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea518f8e-7365-4422-bb3d-67abc7f4f872",
   "metadata": {},
   "source": [
    "# Applications and Use Cases\n",
    "\n",
    "BERT has significantly transformed natural language processing (NLP) by providing a robust pre-trained model applicable to a variety of tasks. It has set new state-of-the-art benchmarks in token-level tasks like named entity recognition and question answering (e.g., SQuAD) and in sentence-level tasks such as sentiment analysis and natural language inference. BERT’s architecture, which fuses bidirectional contextual information from text, eliminates the need for extensive task-specific feature engineering. This versatility has made it a go-to solution for both academic and industrial applications, enabling advancements in chatbots, text summarization, and machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4dba1-b590-4a78-85f1-0bbd015708e5",
   "metadata": {},
   "source": [
    "# Challenges and Limitations\n",
    "\n",
    "While BERT is a powerful model, it comes with several challenges. The model's pre-training process is computationally expensive, requiring significant resources such as multiple TPUs over several days. This creates barriers for researchers and developers without access to such infrastructure. Additionally, fine-tuning BERT for domain-specific applications, such as legal or medical text, often demands substantial labeled data, which may not always be readily available. Its size and memory requirements can also hinder deployment in environments with limited computational resources, such as mobile or edge devices. Moreover, the bidirectional masking strategy introduces a pre-training and fine-tuning mismatch, which impacts its learning efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede3a85-24ea-4356-bfbd-7a1790dbc21d",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "Future research directions for BERT include improving its computational efficiency and scalability. Efforts are underway to develop lightweight versions, such as distillation-based models, that retain performance while reducing resource demands. Expanding BERT’s adaptability to low-resource languages and domains through more effective transfer learning techniques is another promising area. Moreover, advancements in fine-tuning strategies could enable better utilization of small, task-specific datasets, making the model accessible to a broader range of applications. Finally, integrating BERT with other modalities, such as vision or speech, represents a frontier for creating more versatile AI systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347be505-a2a0-4c4f-8ee1-03ab25cd6c11",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "BERT marks a pivotal advancement in NLP, showcasing the power of bidirectional pre-trained models in solving complex language tasks. Its ability to generalize across diverse applications with minimal task-specific adjustments has redefined the field. However, challenges such as high computational costs and domain-specific limitations highlight the need for further innovation. Addressing these limitations while expanding BERT’s applicability will continue to shape its role in the future of NLP and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede1359-886b-46be-b5ac-c05733deb646",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Bert Github Repository: [BERT GitHub Repository](https://github.com/google-research/bert)\n",
    "\n",
    "MRPC Data: [MRPC Data Github Repository](https://github.com/MegEngine/Models/tree/master/official/nlp/bert/glue_data/MRPC)\n",
    "\n",
    "Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffd3ca-b7d5-4e4f-aaaa-ba1b791db3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
