{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c276c4a5-a784-4344-8082-92d38d8fcd05",
   "metadata": {},
   "source": [
    "# The Annotated BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "**Authors:** [Your Name]\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) has transformed the NLP landscape by introducing a bidirectional approach that effectively captures language context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66030aff-0cee-441a-a653-efe7a3e57967",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Preliminaries](#Preliminaries)\n",
    "3. [Background](#Background)\n",
    "4. [Model Architecture](#Model-Architecture)\n",
    "    - [Input Representation](#Input-Representation)\n",
    "    - [Pre-training Objectives](#Pre-training-Objectives)\n",
    "    - [Transformer Encoder Architecture](#Transformer-Encoder-Architecture)\n",
    "    - [Fine-tuning BERT for Classification Tasks](#Fine-tuning-BERT-for-Classification-Tasks)\n",
    "    - [Loading Pre-trained Weights and Fine-tuning](#Loading-Pre-trained-Weights-and-Fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de78808-7acf-4760-b896-9147ef611140",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of Natural Language Processing (NLP) by providing a pre-trained model capable of understanding the context of words in a sentence from both directions (left-to-right and right-to-left). Unlike traditional NLP models, which process text in a unidirectional manner, BERT utilizes the Transformer architecture to create bidirectional representations, making it more powerful in capturing the nuances of language. \n",
    "\n",
    "BERT’s success stems from its innovative pre-training tasks—**Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**—which allow the model to learn rich, deep contextual embeddings from a large corpus of text. After pre-training on vast amounts of unlabelled text, BERT can be fine-tuned for a wide range of NLP tasks, such as question answering, sentiment analysis, and named entity recognition, by simply adding a task-specific layer and fine-tuning on labeled data.\n",
    "\n",
    "This notebook delves into the key concepts behind BERT’s architecture, training methods, and practical applications, with code implementations that demonstrate how BERT can be used to solve real-world NLP challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131326e9-80d9-4955-b4e1-aa46f167e769",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "In this notebook, we will explore the BERT (Bidirectional Encoder Representations from Transformers) model, a breakthrough in Natural Language Processing (NLP) that leverages the Transformer architecture to produce contextualized word embeddings. BERT is pre-trained on a large corpus of text and fine-tuned for downstream tasks such as question answering, sentiment analysis, and named entity recognition.\n",
    "\n",
    "To implement BERT and explore its architecture, we will be using the following Python libraries:\n",
    "\n",
    "1. **Transformers**: A popular library by Hugging Face that provides pre-trained models and tokenizers for state-of-the-art NLP architectures, including BERT.\n",
    "2. **Torch**: A deep learning framework used to run the BERT model and perform tensor computations efficiently on both CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180fbbb-5068-4b03-83de-21726e48c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForNextSentencePrediction, pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import six\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d097c29-1b48-4b19-bf33-d09b3ba94378",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "The field of Natural Language Processing (NLP) has made significant progress over the past decade, largely driven by the development of deep learning models. Prior to the advent of transformer-based models, NLP systems were heavily reliant on traditional models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), which were sequential in nature. While these models were effective for many tasks, they struggled with handling long-range dependencies and parallelization. The breakthrough came with the introduction of the Transformer model by Vaswani et al. in 2017 in the paper \"Attention Is All You Need.\" This model, based on self-attention mechanisms, was capable of processing entire sequences in parallel, making it more efficient and scalable compared to RNNs and LSTMs.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. in 2018, represents a significant advancement in this paradigm. Unlike previous transformer models, which were either unidirectional (left-to-right or right-to-left), BERT leverages a bidirectional approach. This means that it considers context from both the left and right of a token during training, allowing for a deeper understanding of word meaning based on its surrounding context. BERT's architecture is pre-trained on vast amounts of text data using two objectives: **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**. These pre-training tasks enable BERT to capture rich contextual representations of words and their relationships in a sentence, setting it apart from earlier models.\n",
    "\n",
    "The impact of BERT on the NLP community has been profound. It achieved state-of-the-art results across a wide range of benchmarks, including question answering, sentiment analysis, and named entity recognition, among others. BERT’s pre-training approach allows it to be fine-tuned on downstream tasks with relatively small datasets, making it highly versatile for various NLP applications. Additionally, BERT has inspired several model variants, including RoBERTa, ALBERT, and DistilBERT, which build upon and optimize its architecture.\n",
    "\n",
    "With the rise of transformer-based models like BERT, the landscape of NLP research and applications has shifted towards pre-trained models, enabling researchers and developers to fine-tune a single model for a wide range of specific tasks. This approach has significantly reduced the barriers to entry for building state-of-the-art NLP systems, democratizing access to powerful language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044328f0-07ed-473d-a0b9-004581ef0f85",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is based on the Transformer architecture, specifically utilizing the **encoder stack**. Unlike traditional models that process text sequentially (e.g., RNNs or LSTMs), BERT leverages **self-attention mechanisms** that allow it to consider the relationships between all words in a sentence simultaneously, capturing long-range dependencies more efficiently. The bidirectional nature of BERT means that, unlike earlier models which only process text in a left-to-right or right-to-left manner, BERT takes both the left and right context into account during training. This results in richer and more accurate contextual embeddings for words. The Transformer encoder consists of multiple layers of attention heads, followed by position-wise feed-forward networks, enabling the model to learn complex relationships and representations. BERT uses **positional encodings** to retain the order of words in the sentence, which is essential for understanding the sequence in which the words appear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7666e6-4dee-424f-b4e0-c5a460407b3a",
   "metadata": {},
   "source": [
    "## Input Representation\n",
    "BERT’s input representation enables it to process both single sentences and pairs of sentences in a single token sequence. Each token in the input is represented by summing three types of embeddings: Token Embeddings, Segment Embeddings, and Position Embeddings. The token embeddings come from a WordPiece vocabulary of 30,000 tokens, which allows BERT to handle out-of-vocabulary words by breaking them into subword units. Segment embeddings distinguish between different sentences in a sentence-pair input, marking each token as belonging to either sentence A or B. Position embeddings indicate each token's position in the sequence, helping the model understand word order. This combination is crucial for BERT to process various types of tasks and input structures, as it allows the model to interpret both single-sentence and paired-sentence tasks (e.g., question answering) within the same framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085e389-259b-4b66-84e3-626d70f827e0",
   "metadata": {},
   "source": [
    "The input representation process integrates components from `modeling.py`, `tokenization.py`, and task-specific scripts like `run_classifier.py` and `run_squad.py`. `modeling.py` defines the model architecture, including token, segment, and position embeddings, while `tokenization.py` handles the conversion of raw text into WordPiece tokens. In `run_classifier.py` and `run_squad.py`, input processing is managed, specifically for single and paired sentences, ensuring that data is properly tokenized and formatted for different NLP tasks such as classification and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd039a3-7f47-4580-b1ea-adee30bdd490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertModel: Core BERT model class with embeddings and transformer layers\n",
    "\n",
    "class BertModel(object):\n",
    "    \"\"\"BERT model with Token, Segment, and Position Embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=False):\n",
    "        config = copy.deepcopy(config)\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "        self.embedding_output, self.embedding_table = embedding_lookup(\n",
    "            input_ids, config.vocab_size, config.hidden_size, use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "        self.embedding_output = embedding_postprocessor(\n",
    "            self.embedding_output, token_type_ids, config.type_vocab_size)\n",
    "\n",
    "        attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)\n",
    "\n",
    "        self.all_encoder_layers = transformer_model(\n",
    "            input_tensor=self.embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_hidden_layers=config.num_hidden_layers,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=config.attention_probs_dropout_prob\n",
    "        )\n",
    "        self.sequence_output = self.all_encoder_layers[-1]\n",
    "\n",
    "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "        self.pooled_output = tf.layers.dense(\n",
    "            first_token_tensor, config.hidden_size, activation=tf.tanh)\n",
    "\n",
    "    def get_pooled_output(self):\n",
    "        return self.pooled_output\n",
    "\n",
    "    def get_sequence_output(self):\n",
    "        return self.sequence_output\n",
    "\n",
    "    def get_embedding_output(self):\n",
    "        return self.embedding_output\n",
    "\n",
    "    def get_embedding_table(self):\n",
    "        return self.embedding_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292730e0-35dd-4030-949d-2d23241b139a",
   "metadata": {},
   "source": [
    "The `FullTokenizer` combines the `BasicTokenizer` and `WordpieceTokenizer` for complete tokenization. The `BasicTokenizer` handles lowercasing, cleaning, and whitespace splitting, while the `WordpieceTokenizer` splits words into subwords based on a vocabulary, using `[UNK]` for out-of-vocabulary tokens. This process ensures efficient text tokenization for NLP tasks, addressing both basic and subword tokenization needs seamlessly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26651abc-2c78-4b05-878a-2bb73680508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FullTokenizer:\n",
    "    \"\"\"Combines Basic and WordPiece tokenization.\"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.basic_tokenizer = BasicTokenizer()\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self.basic_tokenizer.tokenize(text)\n",
    "        return [sub_token for token in tokens for sub_token in self.wordpiece_tokenizer.tokenize(token)]\n",
    "\n",
    "class BasicTokenizer:\n",
    "    \"\"\"Basic tokenization for text preprocessing.\"\"\"\n",
    "    def tokenize(self, text):\n",
    "        text = convert_to_unicode(text).lower()\n",
    "        text = clean_text(text)\n",
    "        return whitespace_tokenize(text)\n",
    "\n",
    "class WordpieceTokenizer:\n",
    "    \"\"\"Handles WordPiece tokenization.\"\"\"\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "            sub_tokens, start = [], 0\n",
    "            while start < len(chars):\n",
    "                end, cur_substr = len(chars), None\n",
    "                while start < end:\n",
    "                    substr = \"##\" + \"\".join(chars[start:end]) if start > 0 else \"\".join(chars[start:end])\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr:\n",
    "                    sub_tokens.append(cur_substr)\n",
    "                    start = end\n",
    "                else:\n",
    "                    output_tokens.append(self.unk_token)\n",
    "                    break\n",
    "            output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cbeef-a853-4ddd-9a0e-c35925b59edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2231cfa6-d6f0-4a31-943a-a21a44d7c59c",
   "metadata": {},
   "source": [
    "## Pre-training Objectives\n",
    "BERT employs two novel pre-training objectives to learn bidirectional representations: Masked Language Model (MLM) and Next Sentence Prediction (NSP). The MLM task involves randomly masking 15% of the tokens in each input sequence, and the model is then trained to predict these masked tokens based on the surrounding context. This approach allows BERT to leverage context from both left and right sides of each token, unlike traditional unidirectional language models. The NSP task, on the other hand, is designed to improve BERT's understanding of sentence relationships. In this task, pairs of sentences are presented to the model, and it must predict whether the second sentence follows the first in the original text. These two objectives together allow BERT to capture both token-level and sentence-level information, providing a more comprehensive understanding of language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a80c0c-8af7-4036-ade8-9cd66dcacca9",
   "metadata": {},
   "source": [
    "The provided script below is designed for preprocessing text data to create training examples for BERT, particularly for the Masked Language Model (MLM) objective. It includes functions to tokenize input text, randomly mask a portion of tokens, and generate the positions and labels for those masked tokens. The script handles pairs of sequences and ensures their total length does not exceed a specified maximum. It reads raw text from input files, tokenizes it using a pre-trained vocabulary, and generates masked sequences where certain tokens are replaced with the MASK token, a random word, or left unchanged. The final preprocessed data is written to output TFRecord files, which can then be used for training BERT models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f017e5-4259-4275-b21a-63e456b1ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token != \"[CLS]\" and token != \"[SEP]\":\n",
    "            cand_indexes.append(i)\n",
    "\n",
    "    rng.shuffle(cand_indexes)\n",
    "    num_to_mask = min(max_predictions_per_seq, int(round(len(cand_indexes) * masked_lm_prob)))\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "\n",
    "    for i in range(num_to_mask):\n",
    "        masked_index = cand_indexes[i]\n",
    "        masked_lm_positions.append(masked_index)\n",
    "\n",
    "        masked_token = \"[MASK]\"\n",
    "        original_token = tokens[masked_index]\n",
    "\n",
    "        if rng.random() < 0.8:\n",
    "            tokens[masked_index] = masked_token\n",
    "        elif rng.random() < 0.5:\n",
    "            random_word = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "            tokens[masked_index] = random_word\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        masked_lm_labels.append(original_token)\n",
    "\n",
    "    return tokens, masked_lm_positions, masked_lm_labels\n",
    "\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncate a pair of sequences to a maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def main():\n",
    "    rng = random.Random(FLAGS.random_seed)\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "    input_files = FLAGS.input_file.split(\",\")\n",
    "    instances = create_training_instances(input_files, tokenizer, FLAGS.max_seq_length,\n",
    "                                          FLAGS.dupe_factor, FLAGS.short_seq_prob,\n",
    "                                          FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq, rng)\n",
    "    output_files = FLAGS.output_file.split(\",\")\n",
    "    write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                    FLAGS.max_predictions_per_seq, output_files)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f578240-8ce9-4aff-8cbf-b76a51285812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTPreTraining:\n",
    "    def __init__(self, bert_config, init_checkpoint, learning_rate, num_train_steps,\n",
    "                 num_warmup_steps, use_tpu, use_one_hot_embeddings):\n",
    "        self.bert_config = bert_config\n",
    "        self.init_checkpoint = init_checkpoint\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.use_tpu = use_tpu\n",
    "        self.use_one_hot_embeddings = use_one_hot_embeddings\n",
    "\n",
    "    def model_fn_builder(self):\n",
    "        \"\"\"Returns model_fn closure for TPUEstimator.\"\"\"\n",
    "        def model_fn(features, labels, mode, params):\n",
    "            tf.logging.info(\"*** Features ***\")\n",
    "            for name in sorted(features.keys()):\n",
    "                tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "            input_ids = features[\"input_ids\"]\n",
    "            input_mask = features[\"input_mask\"]\n",
    "            segment_ids = features[\"segment_ids\"]\n",
    "            masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "            masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "            masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "            next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "\n",
    "            is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "            model = modeling.BertModel(\n",
    "                config=self.bert_config,\n",
    "                is_training=is_training,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                token_type_ids=segment_ids,\n",
    "                use_one_hot_embeddings=self.use_one_hot_embeddings)\n",
    "\n",
    "            masked_lm_loss, _, _ = self.get_masked_lm_output(\n",
    "                model.get_sequence_output(), masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "            next_sentence_loss, _, _ = self.get_next_sentence_output(\n",
    "                model.get_pooled_output(), next_sentence_labels)\n",
    "\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            initialized_variable_names = {}\n",
    "            scaffold_fn = None\n",
    "            if self.init_checkpoint:\n",
    "                assignment_map, initialized_variable_names = modeling.get_assignment_map_from_checkpoint(\n",
    "                    tvars, self.init_checkpoint)\n",
    "                if self.use_tpu:\n",
    "                    def tpu_scaffold():\n",
    "                        tf.train.init_from_checkpoint(self.init_checkpoint, assignment_map)\n",
    "                        return tf.train.Scaffold()\n",
    "                    scaffold_fn = tpu_scaffold\n",
    "                else:\n",
    "                    tf.train.init_from_checkpoint(self.init_checkpoint, assignment_map)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                train_op = optimization.create_optimizer(\n",
    "                    total_loss, self.learning_rate, self.num_train_steps, self.num_warmup_steps, self.use_tpu)\n",
    "                return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=total_loss, train_op=train_op, scaffold_fn=scaffold_fn)\n",
    "            elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "                eval_metrics = self.get_eval_metrics(masked_lm_loss, masked_lm_ids, masked_lm_weights, next_sentence_loss)\n",
    "                return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=total_loss, eval_metrics=eval_metrics, scaffold_fn=scaffold_fn)\n",
    "            else:\n",
    "                raise ValueError(\"Only TRAIN and EVAL modes are supported\")\n",
    "\n",
    "        return model_fn\n",
    "\n",
    "    def get_masked_lm_output(self, input_tensor, positions, label_ids, label_weights):\n",
    "        \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
    "        input_tensor = self.gather_indexes(input_tensor, positions)\n",
    "        with tf.variable_scope(\"cls/predictions\"):\n",
    "            input_tensor = tf.layers.dense(\n",
    "                input_tensor,\n",
    "                units=self.bert_config.hidden_size,\n",
    "                activation=modeling.get_activation(self.bert_config.hidden_act),\n",
    "                kernel_initializer=modeling.create_initializer(self.bert_config.initializer_range))\n",
    "            input_tensor = modeling.layer_norm(input_tensor)\n",
    "            output_bias = tf.get_variable(\"output_bias\", shape=[self.bert_config.vocab_size], initializer=tf.zeros_initializer())\n",
    "            logits = tf.matmul(input_tensor, self.bert_config.embedding_table, transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "            label_ids = tf.reshape(label_ids, [-1])\n",
    "            label_weights = tf.reshape(label_weights, [-1])\n",
    "            one_hot_labels = tf.one_hot(label_ids, depth=self.bert_config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "            per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "            numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
    "            denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "            loss = numerator / denominator\n",
    "\n",
    "        return loss, per_example_loss, log_probs\n",
    "\n",
    "    def get_next_sentence_output(self, input_tensor, labels):\n",
    "        \"\"\"Get loss and log probs for the next sentence prediction.\"\"\"\n",
    "        with tf.variable_scope(\"cls/seq_relationship\"):\n",
    "            output_weights = tf.get_variable(\"output_weights\", shape=[2, self.bert_config.hidden_size],\n",
    "                                             initializer=modeling.create_initializer(self.bert_config.initializer_range))\n",
    "            output_bias = tf.get_variable(\"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n",
    "            logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "            labels = tf.reshape(labels, [-1])\n",
    "            one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
    "            per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "            loss = tf.reduce_mean(per_example_loss)\n",
    "        return loss, per_example_loss, log_probs\n",
    "\n",
    "    def gather_indexes(self, sequence_tensor, positions):\n",
    "        \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "        sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "        batch_size = sequence_shape[0]\n",
    "        seq_length = sequence_shape[1]\n",
    "        width = sequence_shape[2]\n",
    "        flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "        flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "        flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n",
    "        return tf.gather(flat_sequence_tensor, flat_positions)\n",
    "\n",
    "    def get_eval_metrics(self, masked_lm_loss, masked_lm_ids, masked_lm_weights, next_sentence_loss):\n",
    "        \"\"\"Computes the loss and accuracy of the model.\"\"\"\n",
    "        masked_lm_log_probs = tf.reshape(masked_lm_log_probs, [-1, masked_lm_log_probs.shape[-1]])\n",
    "        masked_lm_predictions = tf.argmax(masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
    "        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n",
    "        masked_lm_accuracy = tf.metrics.accuracy(labels=masked_lm_ids, predictions=masked_lm_predictions, weights=masked_lm_weights)\n",
    "        masked_lm_mean_loss = tf.metrics.mean(values=masked_lm_example_loss, weights=masked_lm_weights)\n",
    "\n",
    "        next_sentence_log_probs = tf.reshape(next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n",
    "        next_sentence_predictions = tf.argmax(next_sentence_log_probs, axis=-1, output_type=tf.int32)\n",
    "        next_sentence_accuracy = tf.metrics.accuracy(labels=next_sentence_labels, predictions=next_sentence_predictions)\n",
    "        next_sentence_mean_loss = tf.metrics.mean(values=next_sentence_example_loss)\n",
    "\n",
    "        return {\n",
    "            \"masked_lm_accuracy\": masked_lm_accuracy,\n",
    "            \"masked_lm_loss\": masked_lm_mean_loss,\n",
    "            \"next_sentence_accuracy\": next_sentence_accuracy,\n",
    "            \"next_sentence_loss\": next_sentence_mean_loss,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28748245-8c40-4dea-ac93-393f8e9b032f",
   "metadata": {},
   "source": [
    "##  Transformer Encoder Architecture\n",
    "Each Transformer layer contains two main components: Multi-Head Self-Attention and Feed-Forward Neural Network (FFN). The multi-head self-attention mechanism allows each token to attend to all other tokens in the sequence, capturing dependencies across long ranges and enhancing the model’s contextual understanding. Following the self-attention, a feed-forward network processes each token representation independently. Layer normalization and dropout are applied after each component to stabilize and regularize training. BERT’s bidirectional structure enables each token to attend to both preceding and following tokens, distinguishing it from previous models like GPT, which used left-to-right unidirectional attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668e620-6355-458c-b949-f52b7a02452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel:\n",
    "    def __init__(self, config, is_training, input_ids):\n",
    "        with tf.variable_scope(\"bert\"):\n",
    "            self.embedding_output = embedding_layer(input_ids, config)\n",
    "            self.all_encoder_layers = transformer_stack(\n",
    "                self.embedding_output,\n",
    "                hidden_size=config.hidden_size,\n",
    "                num_hidden_layers=config.num_hidden_layers,\n",
    "                num_attention_heads=config.num_attention_heads,\n",
    "                intermediate_size=config.intermediate_size\n",
    "            )\n",
    "            self.sequence_output = self.all_encoder_layers[-1]\n",
    "\n",
    "def transformer_stack(input_tensor, hidden_size, num_hidden_layers, num_attention_heads, intermediate_size):\n",
    "    prev_output = input_tensor\n",
    "    all_layers = []\n",
    "    for layer_idx in range(num_hidden_layers):\n",
    "        with tf.variable_scope(f\"layer_{layer_idx}\"):\n",
    "            layer_output = transformer_layer(\n",
    "                prev_output, hidden_size, num_attention_heads, intermediate_size\n",
    "            )\n",
    "            all_layers.append(layer_output)\n",
    "            prev_output = layer_output\n",
    "    return all_layers\n",
    "\n",
    "def transformer_layer(input_tensor, hidden_size, num_attention_heads, intermediate_size):\n",
    "    attention_output = multi_head_attention(input_tensor, hidden_size, num_attention_heads)\n",
    "    attention_output = layer_norm(input_tensor + attention_output)\n",
    "    intermediate_output = ff_layer(attention_output, intermediate_size)\n",
    "    layer_output = layer_norm(attention_output + intermediate_output)\n",
    "    return layer_output\n",
    "\n",
    "def multi_head_attention(input_tensor, hidden_size, num_attention_heads):\n",
    "    pass\n",
    "\n",
    "def ff_layer(input_tensor, intermediate_size):\n",
    "    pass\n",
    "\n",
    "def layer_norm(input_tensor):\n",
    "    pass\n",
    "\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292f304-1ff6-4ff0-ad8f-6a2966507bc3",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT for Classification Tasks\n",
    "To adapt BERT for specific downstream tasks, a task-specific layer is added to the output of BERT, typically using the hidden state of the CLS token. For classification tasks, this hidden state is fed into a classification layer, where a fully connected layer maps it to the appropriate number of output labels. The entire model, including BERT’s pre-trained parameters, is then fine-tuned on the task-specific dataset by optimizing a loss function (e.g., cross-entropy for classification). This approach allows BERT to effectively apply its pre-trained knowledge to a wide range of NLP tasks with minimal task-specific architecture modifications, demonstrating the flexibility and transferability of the BERT model across different tasks such as sentence classification, sentiment analysis, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b08cb15a-d551-41e6-b00f-aa23ac128d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    output_layer = model.get_pooled_output()\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "    return (loss, per_example_loss, logits, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc373704-6886-43ff-81f0-7b20cbdced1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(object):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 is_training,\n",
    "                 input_ids,\n",
    "                 input_mask=None,\n",
    "                 token_type_ids=None,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 scope=None):\n",
    "        # ... (initialization code)\n",
    "\n",
    "    def get_pooled_output(self):\n",
    "        return self.pooled_output\n",
    "\n",
    "    def get_sequence_output(self):\n",
    "        return self.sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0c918-05a9-4ce2-a959-d1a09bf9a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
    "    learning_rate = tf.train.polynomial_decay(\n",
    "        learning_rate,\n",
    "        global_step,\n",
    "        num_train_steps,\n",
    "        end_learning_rate=0.0,\n",
    "        power=1.0,\n",
    "        cycle=False)\n",
    "\n",
    "    if num_warmup_steps:\n",
    "        global_steps_int = tf.cast(global_step, tf.int32)\n",
    "        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
    "\n",
    "        global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "        warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "        warmup_learning_rate = init_lr * warmup_percent_done\n",
    "\n",
    "        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "        learning_rate = (\n",
    "            (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "\n",
    "    optimizer = AdamWeightDecayOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay_rate=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-6,\n",
    "        exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
    "\n",
    "    if use_tpu:\n",
    "        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "\n",
    "    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "    new_global_step = global_step + 1\n",
    "    train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
    "    return train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e350b2e-65ad-4ff6-a901-bcb937069a06",
   "metadata": {},
   "source": [
    "## Loading Pre-trained Weights and Fine-tuning\n",
    "During fine-tuning, pre-trained weights from the BERT model are loaded into the model, which are then further optimized on the downstream task data. These weights are trained using large corpora, such as the BooksCorpus and English Wikipedia, which contain millions of words. By initializing BERT with these weights, the model can leverage the representations learned during pre-training, allowing it to converge faster and achieve better performance with less data. In the fine-tuning phase, all of BERT’s parameters are adjusted based on the specific task, making it highly adaptable to various NLP applications. This transfer learning approach has proven highly effective for improving performance across multiple tasks, as BERT’s pre-trained knowledge provides a strong foundation that task-specific fine-tuning can refine further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4cdd28-7a59-4124-88c7-de16ad7ee627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
    "    def __init__(self, learning_rate, weight_decay_rate=0.0, beta_1=0.9, beta_2=0.999, epsilon=1e-6, exclude_from_weight_decay=None, name=\"AdamWeightDecayOptimizer\"):\n",
    "        super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay_rate = weight_decay_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "\n",
    "    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "        assignments = []\n",
    "        for (grad, param) in grads_and_vars:\n",
    "            if grad is None or param is None:\n",
    "                continue\n",
    "            param_name = self._get_variable_name(param.name)\n",
    "            m = tf.get_variable(name=param_name + \"/adam_m\", shape=param.shape.as_list(), dtype=tf.float32, trainable=False, initializer=tf.zeros_initializer())\n",
    "            v = tf.get_variable(name=param_name + \"/adam_v\", shape=param.shape.as_list(), dtype=tf.float32, trainable=False, initializer=tf.zeros_initializer())\n",
    "            next_m = tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad)\n",
    "            next_v = tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2, tf.square(grad))\n",
    "            update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
    "            if self._do_use_weight_decay(param_name):\n",
    "                update += self.weight_decay_rate * param\n",
    "            update_with_lr = self.learning_rate * update\n",
    "            next_param = param - update_with_lr\n",
    "            assignments.extend([param.assign(next_param), m.assign(next_m), v.assign(next_v)])\n",
    "        return tf.group(*assignments, name=name)\n",
    "\n",
    "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
    "    learning_rate = tf.train.polynomial_decay(learning_rate, global_step, num_train_steps, end_learning_rate=0.0, power=1.0, cycle=False)\n",
    "    if num_warmup_steps:\n",
    "        global_steps_int = tf.cast(global_step, tf.int32)\n",
    "        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
    "        global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "        warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "        warmup_learning_rate = init_lr * warmup_percent_done\n",
    "        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "        learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "    optimizer = AdamWeightDecayOptimizer(learning_rate=learning_rate, weight_decay_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-6, exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
    "    if use_tpu:\n",
    "        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "    new_global_step = global_step + 1\n",
    "    train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197f094-d3be-40a0-907e-519d44452402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
